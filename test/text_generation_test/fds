/home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
04/02/2019 14:02:57 - INFO - programmingalpha.models.rl_utility.data_utils -   train/valid data size:130653/2000
04/02/2019 14:02:58 - INFO - programmingalpha.models.RL_Transformer -   vocab size: 30522/30522
04/02/2019 14:02:58 - INFO - __main__ -   traditional training method with mulit-gpu(2)
WARNING:tensorflow:From ./build_rl_transformer.py:612: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
04/02/2019 14:02:58 - WARNING - tensorflow -   From ./build_rl_transformer.py:612: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
04/02/2019 14:02:58 - WARNING - tensorflow -   From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
04/02/2019 14:02:58 - WARNING - tensorflow -   From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
04/02/2019 14:02:58 - INFO - programmingalpha.models.RL_Transformer -   creating bert graph, is training:True
WARNING:tensorflow:From /home/LAB/zhangzy/ProgrammingAlpha/bert/modeling.py:360: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
04/02/2019 14:02:58 - WARNING - tensorflow -   From /home/LAB/zhangzy/ProgrammingAlpha/bert/modeling.py:360: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/LAB/zhangzy/ProgrammingAlpha/bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
04/02/2019 14:02:58 - WARNING - tensorflow -   From /home/LAB/zhangzy/ProgrammingAlpha/bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/LAB/zhangzy/ProgrammingAlpha/texar/texar/modules/embedders/position_embedders.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
04/02/2019 14:03:00 - WARNING - tensorflow -   From /home/LAB/zhangzy/ProgrammingAlpha/texar/texar/modules/embedders/position_embedders.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/LAB/zhangzy/ProgrammingAlpha/texar/texar/modules/decoders/transformer_decoders.py:644: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
04/02/2019 14:03:00 - WARNING - tensorflow -   From /home/LAB/zhangzy/ProgrammingAlpha/texar/texar/modules/decoders/transformer_decoders.py:644: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
04/02/2019 14:03:10 - INFO - programmingalpha.models.RL_Transformer -   creating bert graph, is training:True
04/02/2019 14:03:21 - INFO - __main__ -   n_group:2, var_num:304/306
2019-04-02 14:03:28.039720: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-02 14:03:28.923595: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55719753e1b0 executing computations on platform CUDA. Devices:
2019-04-02 14:03:28.923642: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-04-02 14:03:28.923652: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-04-02 14:03:28.923659: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-04-02 14:03:28.923665: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-04-02 14:03:28.947927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2399965000 Hz
2019-04-02 14:03:28.953564: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5571955576c0 executing computations on platform Host. Devices:
2019-04-02 14:03:28.953618: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-02 14:03:28.954568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-04-02 14:03:28.955238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-04-02 14:03:28.955463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.90GiB freeMemory: 476.88MiB
2019-04-02 14:03:28.955701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:09:00.0
totalMemory: 15.90GiB freeMemory: 476.88MiB
2019-04-02 14:03:28.973303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3
2019-04-02 14:03:28.978210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-02 14:03:28.978230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3
2019-04-02 14:03:28.978239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y Y Y
2019-04-02 14:03:28.978245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N Y Y
2019-04-02 14:03:28.978251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   Y Y N Y
2019-04-02 14:03:28.978256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   Y Y Y N
2019-04-02 14:03:28.979228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
2019-04-02 14:03:28.979718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15190 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
2019-04-02 14:03:28.980181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 251 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
2019-04-02 14:03:28.980621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 251 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)
04/02/2019 14:03:28 - INFO - __main__ -   init vars!
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   *************initing bert*****************
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   current bert variables num: 593
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/word_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/token_type_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/position_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/pooler/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/pooler/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/word_embeddings/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/word_embeddings/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/token_type_embeddings/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/token_type_embeddings/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/position_embeddings/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/position_embeddings/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/kernel/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/kernel/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/bias/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/bias/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/beta/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/beta/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/gamma/adam_m:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/gamma/adam_v:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   ckpt vars num:206
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/embeddings/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/embeddings/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/embeddings/position_embeddings
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/embeddings/token_type_embeddings
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/embeddings/word_embeddings
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_0/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_1/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_10/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_11/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_2/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_3/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_4/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_5/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_6/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_7/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_8/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/key/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/key/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/query/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/query/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/value/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/attention/self/value/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/intermediate/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/intermediate/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/output/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/output/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/output/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/encoder/layer_9/output/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/pooler/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:bert/pooler/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/predictions/output_bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/predictions/transform/LayerNorm/beta
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/predictions/transform/LayerNorm/gamma
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/predictions/transform/dense/bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/predictions/transform/dense/kernel
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/seq_relationship/output_bias
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name:cls/seq_relationship/output_weights
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   restorbale vars num: 199
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/word_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/token_type_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/position_embeddings:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/embeddings/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_0/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_1/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_2/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_3/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_4/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_5/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_6/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_7/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_8/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_9/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_10/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/query/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/key/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/self/value/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/attention/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/intermediate/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/dense/bias:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/beta:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/encoder/layer_11/output/LayerNorm/gamma:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/pooler/dense/kernel:0
04/02/2019 14:03:42 - INFO - programmingalpha.models.RL_Transformer -   var name: bert/pooler/dense/bias:0
WARNING:tensorflow:From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
04/02/2019 14:03:42 - WARNING - tensorflow -   From /home/LAB/zhangzy/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /home/LAB/zhangzy/ShareModels/uncased_L-12_H-768_A-12/bert_model.ckpt
04/02/2019 14:03:42 - INFO - tensorflow -   Restoring parameters from /home/LAB/zhangzy/ShareModels/uncased_L-12_H-768_A-12/bert_model.ckpt
04/02/2019 14:03:45 - INFO - programmingalpha.models.RL_Transformer -   initing bert finished!
04/02/2019 14:03:50 - INFO - __main__ -   Begin running with train_and_evaluate mode
2019-04-02 14:03:52.698316: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
